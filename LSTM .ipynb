{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO-ATrVEp3vX"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "from html import unescape\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI0lAVpgzRou"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAQoDTjTty-j"
      },
      "source": [
        "data = pd.read_csv('Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]\n",
        "data = data[data.sentiment != \"Neutral\"]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O12WoFBt-_V"
      },
      "source": [
        "def clean_sentence(sentence):\n",
        "      emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "      #HappyEmoticons\n",
        "      emoticons_happy = set([\n",
        "      ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "      ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "      '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "      'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "      '<3'\n",
        "      ])\n",
        "      # Sad Emoticons\n",
        "      emoticons_sad = set([\n",
        "      ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "      ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "      ':c', ':{', '>:\\\\', ';('\n",
        "      ])\n",
        "      #combine sad and happy emoticons\n",
        "      emoticons = emoticons_happy.union(emoticons_sad)\n",
        "      # remove mention\n",
        "      sentence=re.sub('@\\w*', '', sentence)\n",
        "      # remove hashtag\n",
        "      sentence = re.sub('#', '', sentence)\n",
        "      # remove retweet\n",
        "      sentence = re.sub('RT[\\s]+', '', sentence) \n",
        "      # remove hyperlink\n",
        "      sentence = re.sub('https?:\\/\\/\\S+', '', sentence)\n",
        "      # remove special string of html content\n",
        "      sentence=BeautifulSoup(unescape(sentence)).text\n",
        "      # remove icon\n",
        "      for icon in emoticons:\n",
        "          sentence=sentence.replace(icon,'')\n",
        "      # remove emoji\n",
        "      sentence=emoji_pattern.sub(r'', sentence)\n",
        "      sentence = re.sub(r\"what's\", \"what is \", sentence)\n",
        "      sentence = re.sub(r\"\\'s\", \" \", sentence)\n",
        "      sentence = re.sub(r\"\\'ve\", \" have \", sentence)\n",
        "      sentence = re.sub(r\"can't\", \"cannot \", sentence)\n",
        "      sentence = re.sub(r\"n't\", \" not \", sentence)\n",
        "      sentence = re.sub(r\"i'm\", \"i am \", sentence)\n",
        "      sentence = re.sub(r\"\\'re\", \" are \", sentence)\n",
        "      sentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
        "      sentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
        "      sentence = re.sub(r\"\\'scuse\", \" excuse \", sentence)\n",
        "      sentence = re.sub('\\W', ' ', sentence)\n",
        "      sentence = re.sub('\\s+', ' ', sentence)\n",
        "      sentence=sentence.strip(' ')\n",
        "      \n",
        "      return sentence"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjI4xfmquJnt"
      },
      "source": [
        "def sen_tokenize(sentence):\n",
        "    #Emoji patterns\n",
        "    tokenizer = RegexpTokenizer(r\"[A-Za-z0-9]\\w*(?:['?]\\w+)?\")\n",
        "    tokens=[]\n",
        "    tokens+=tokenizer.tokenize(clean_sentence(sentence))\n",
        "    # lower case token\n",
        "    tokens=[w.lower() for w in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KXjMshYviXD"
      },
      "source": [
        "def filter_tokens(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens=sen_tokenize(sentence)\n",
        "    filtered_tokens=[]\n",
        "    # remove punctation\n",
        "    for w in tokens:\n",
        "        if w not in string.punctuation and w not in stop_words and len(w)>=3 and not(w.isnumeric()) :\n",
        "            filtered_tokens.append(w)\n",
        "    return filtered_tokens"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6vz73yUv9Ih",
        "outputId": "1861199c-04ed-4b17-88da-f2e68b1a3f8d"
      },
      "source": [
        " import nltk\n",
        " nltk.download('stopwords')\n",
        "for idx in range(len(data)):\n",
        "  data.iloc[idx,0]=filter_tokens(data.iloc[idx,0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMMwICexv_ye"
      },
      "source": [
        "for i in range(len(data)):\n",
        "    data.iloc[i,0]=' '.join(data.iloc[i,0])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5IIhnCE35l6"
      },
      "source": [
        ""
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPTQu_SPvi_V"
      },
      "source": [
        "# Load Pretrained Word2Vec\n",
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vFoOiW3vzqP"
      },
      "source": [
        "def get_word2vec_enc(reviews):\n",
        "    \"\"\"\n",
        "    get word2vec value for each word in sentence.\n",
        "    concatenate word in numpy array, so we can use it as RNN input\n",
        "    \"\"\"\n",
        "    encoded_reviews = []\n",
        "    for review in reviews:\n",
        "        tokens = review.split(\" \")\n",
        "        word2vec_embedding = embed(tokens)\n",
        "        encoded_reviews.append(word2vec_embedding)\n",
        "    return encoded_reviews\n",
        "\n",
        "\n",
        "def get_padded_encoded_reviews(encoded_reviews):\n",
        "    \"\"\"\n",
        "    for short sentences, we prepend zero padding so all input to RNN has same length\n",
        "    \"\"\"\n",
        "    padded_reviews_encoding = []\n",
        "    for enc_review in encoded_reviews:\n",
        "        zero_padding_cnt = 33 - enc_review.shape[0]\n",
        "        pad = np.zeros((1, 250))\n",
        "        for i in range(zero_padding_cnt):\n",
        "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
        "        padded_reviews_encoding.append(enc_review)\n",
        "    return padded_reviews_encoding\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SB1eIDLwcse"
      },
      "source": [
        "X=get_padded_encoded_reviews(get_word2vec_enc(data.text))\n",
        "X=np.array(tf.convert_to_tensor(X, dtype=tf.float32))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oXFy9qOwUXZ",
        "outputId": "539045d7-c52e-4570-95d4-5aea49337c07"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10729, 33, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnExpzY3w34y"
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tfidf_model = TfidfVectorizer(token_pattern=r\"\\w+['\\w]*\",ngram_range=(1,2),min_df=0.01,max_df=0.95) # specify parameters here\n",
        "# tfidf_model.fit(data['text'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwRKkSqkyVhf"
      },
      "source": [
        "# raw_data_tfidf=tfidf_model.transform(data['text'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ef3TINYyWi3"
      },
      "source": [
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z2GyG6SNbpE"
      },
      "source": [
        ""
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjdpop9AylBO"
      },
      "source": [
        "# X=raw_data_tfidf\n",
        "# X=X.toarray()\n",
        "# X=[sorted(X[idx],reverse=True)[:20] for idx in range(len(X))]\n",
        "# X=np.array(X)\n",
        "# X = np.expand_dims(X, 2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybkpC-STZVdn"
      },
      "source": [
        ""
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VaEDZfYODQ8"
      },
      "source": [
        "# X=[sorted(X[idx],reverse=True)[:20] for idx in range(len(X))]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuIPXGPaAVNZ"
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(LSTM(units=64,input_shape=(20, 1 ),dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(2,activation='softmax'))\n",
        "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# print(model.summary())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QLO7VR2Mj3m",
        "outputId": "68b64c0f-07a8-4ac3-d48d-4b5e021947c4"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le=preprocessing.LabelEncoder()\n",
        "Y=le.fit_transform(data['sentiment'])\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7188, 29) (7188,)\n",
            "(3541, 29) (3541,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "pdBOQLS5P7Db",
        "outputId": "44506db4-1fb4-4648-dd1d-e320597c4eba"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "X_train=pd.DataFrame(data=X_train[0:,0:],\n",
        "    index=[i for i in range(X_train.shape[0])],\n",
        "    columns=['f'+str(i) for i in range(X_train.shape[1])])\n",
        "Y_train=pd.DataFrame(data=Y_train)\n",
        "\n",
        "df = pd.concat([X_train, Y_train], axis=1)\n",
        "\n",
        "\n",
        "# Separate majority and minority classes\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df.iloc[:,18]==0]\n",
        "df_minority = df[df.iloc[:,18]==1]\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=1234) # reproducible results\n",
        " \n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        " \n",
        "# Display new class counts\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-7a40b878623e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train=pd.DataFrame(data=X_train[0:,0:],\n\u001b[1;32m      3\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     columns=['f'+str(i) for i in range(X_train.shape[1])])\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(7188, 33, 250)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HEsqqSMx56a"
      },
      "source": [
        "X_train=np.array(df_upsampled.iloc[:,:18])\n",
        "Y_train=np.array(df_upsampled.iloc[:,18])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E-w9MgbSdHz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQoPMV2bMkEI"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoPDgxWUAY5z",
        "outputId": "c2bee85f-f3b6-45cc-f121-660822aaf40a"
      },
      "source": [
        "embed_dim = 128\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2,input_shape=(29,128)))\n",
        "model.add(Dense(128,kernel_initializer='HeNormal', input_dim=196,activation='relu'))\n",
        "model.add(Dense(64,kernel_initializer='HeNormal', input_dim=128,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 29, 128)           256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 29, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               25216     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 544,337\n",
            "Trainable params: 544,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6APbYRKFK1a5",
        "outputId": "101dc7f4-2eaa-4008-b7bd-da70792e1041"
      },
      "source": [
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "batch_size = 64\n",
        "\n",
        "checkpoint = ModelCheckpoint('model_best_weights.h5', monitor='val_accuracy', verbose=2, save_best_only=True, mode='max', period=1)\n",
        "\n",
        "model.fit(X_train, Y_train, epochs =10,validation_data = (X_test, \n",
        "Y_test), callbacks=[checkpoint] ,batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "113/113 - 16s - loss: 0.4561 - accuracy: 0.8043 - val_loss: 0.3781 - val_accuracy: 0.8376\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.83762, saving model to model_best_weights.h5\n",
            "Epoch 2/10\n",
            "113/113 - 14s - loss: 0.3232 - accuracy: 0.8623 - val_loss: 0.3540 - val_accuracy: 0.8563\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.83762 to 0.85626, saving model to model_best_weights.h5\n",
            "Epoch 3/10\n",
            "113/113 - 14s - loss: 0.2835 - accuracy: 0.8836 - val_loss: 0.3381 - val_accuracy: 0.8565\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.85626 to 0.85654, saving model to model_best_weights.h5\n",
            "Epoch 4/10\n",
            "113/113 - 14s - loss: 0.2488 - accuracy: 0.8964 - val_loss: 0.3574 - val_accuracy: 0.8577\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.85654 to 0.85767, saving model to model_best_weights.h5\n",
            "Epoch 5/10\n",
            "113/113 - 14s - loss: 0.2266 - accuracy: 0.9083 - val_loss: 0.3991 - val_accuracy: 0.8591\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.85767 to 0.85908, saving model to model_best_weights.h5\n",
            "Epoch 6/10\n",
            "113/113 - 13s - loss: 0.2074 - accuracy: 0.9150 - val_loss: 0.4122 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.85908\n",
            "Epoch 7/10\n",
            "113/113 - 14s - loss: 0.1870 - accuracy: 0.9263 - val_loss: 0.4398 - val_accuracy: 0.8430\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.85908\n",
            "Epoch 8/10\n",
            "113/113 - 13s - loss: 0.1658 - accuracy: 0.9329 - val_loss: 0.5259 - val_accuracy: 0.8229\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.85908\n",
            "Epoch 9/10\n",
            "113/113 - 14s - loss: 0.1534 - accuracy: 0.9349 - val_loss: 0.5182 - val_accuracy: 0.8354\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.85908\n",
            "Epoch 10/10\n",
            "113/113 - 13s - loss: 0.1426 - accuracy: 0.9424 - val_loss: 0.5076 - val_accuracy: 0.8410\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.85908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8a7062d940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV8E7LHyUNew",
        "outputId": "251d53f6-dbba-4afc-b9d8-ce1bbba7b9fd"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.models import load_model\n",
        "\n",
        "# load model\n",
        "model = load_model('model_best_weights.h5')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi-lTRaRt0G5",
        "outputId": "24a63ce6-8dcd-4c2b-f8a2-4e9516e6d5b6"
      },
      "source": [
        "y_pred=model.predict_classes(X_test)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKhZNaE4uziY",
        "outputId": "cecd6376-6bc9-4c4f-8b09-90bd7fee5d6d"
      },
      "source": [
        "roc_auc_score(Y_test,y_pred)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7450170349363898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY7BTfEPu6yD",
        "outputId": "0c7d7111-b87a-48d9-9a38-1db78f4fc22f"
      },
      "source": [
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56/56 - 1s - loss: 0.3991 - accuracy: 0.8591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAOcp6gk7x5Y",
        "outputId": "a5afb6c8-158c-4a32-9c76-1061d1dc594c"
      },
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for idx in range(len(X_test)):\n",
        "    \n",
        "    result = model.predict_classes(X_test[idx].reshape(1,29))[0][0]\n",
        "   \n",
        "    if result == Y_test[idx]:\n",
        "        if result == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if Y_test[idx] == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBmUnWgA7zfT",
        "outputId": "16c0a879-415b-4553-c9fc-ccc7c706db51"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3541, 33, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA9HF5smHYkU"
      },
      "source": [
        ""
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HziWItjeIdDL",
        "outputId": "7e0e30c8-9f52-43e9-8edf-ff6ea70bd546"
      },
      "source": [
        ""
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1BiQb2gIjDJ"
      },
      "source": [
        ""
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHPOOyoHJUnq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}