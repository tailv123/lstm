{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO-ATrVEp3vX"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "from html import unescape\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAQoDTjTty-j"
      },
      "source": [
        "data = pd.read_csv('IMDB Dataset.csv')\n",
        "# Keeping only the neccessary columns\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(data.review,data.sentiment, test_size = 0.1, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EYTsJ6ASSBW"
      },
      "source": [
        "df=pd.DataFrame([],columns=['review','sentiment'])\n",
        "df.review=X_train\n",
        "df.sentiment=Y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44GTGXLjSqN0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O12WoFBt-_V"
      },
      "source": [
        "def clean_sentence(sentence):\n",
        "      emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "      #HappyEmoticons\n",
        "      emoticons_happy = set([\n",
        "      ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "      ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "      '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "      'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "      '<3'\n",
        "      ])\n",
        "      # Sad Emoticons\n",
        "      emoticons_sad = set([\n",
        "      ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "      ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "      ':c', ':{', '>:\\\\', ';('\n",
        "      ])\n",
        "      #combine sad and happy emoticons\n",
        "      emoticons = emoticons_happy.union(emoticons_sad)\n",
        "      # remove mention\n",
        "      sentence=re.sub('@\\w*', '', sentence)\n",
        "      # remove hashtag\n",
        "      sentence = re.sub('#', '', sentence)\n",
        "      # remove retweet\n",
        "      sentence = re.sub('RT[\\s]+', '', sentence) \n",
        "      # remove hyperlink\n",
        "      sentence = re.sub('https?:\\/\\/\\S+', '', sentence)\n",
        "      # remove special string of html content\n",
        "      sentence=BeautifulSoup(unescape(sentence)).text\n",
        "      # remove icon\n",
        "      for icon in emoticons:\n",
        "          sentence=sentence.replace(icon,'')\n",
        "      # remove emoji\n",
        "      sentence=emoji_pattern.sub(r'', sentence)\n",
        "      sentence = re.sub(r\"what's\", \"what is \", sentence)\n",
        "      sentence = re.sub(r\"\\'s\", \" \", sentence)\n",
        "      sentence = re.sub(r\"\\'ve\", \" have \", sentence)\n",
        "      sentence = re.sub(r\"can't\", \"cannot \", sentence)\n",
        "      sentence = re.sub(r\"n't\", \" not \", sentence)\n",
        "      sentence = re.sub(r\"i'm\", \"i am \", sentence)\n",
        "      sentence = re.sub(r\"\\'re\", \" are \", sentence)\n",
        "      sentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
        "      sentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
        "      sentence = re.sub(r\"\\'scuse\", \" excuse \", sentence)\n",
        "      sentence = re.sub('\\W', ' ', sentence)\n",
        "      sentence = re.sub('\\s+', ' ', sentence)\n",
        "      sentence=sentence.strip(' ')\n",
        "      \n",
        "      return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjI4xfmquJnt"
      },
      "source": [
        "def sen_tokenize(sentence):\n",
        "    #Emoji patterns\n",
        "    tokenizer = RegexpTokenizer(r\"[A-Za-z0-9]\\w*(?:['?]\\w+)?\")\n",
        "    tokens=[]\n",
        "    tokens+=tokenizer.tokenize(clean_sentence(sentence))\n",
        "    # lower case token\n",
        "    tokens=[w.lower() for w in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KXjMshYviXD"
      },
      "source": [
        "def filter_tokens(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens=sen_tokenize(sentence)\n",
        "    filtered_tokens=[]\n",
        "    # remove punctation\n",
        "    for w in tokens:\n",
        "        if w not in string.punctuation and w not in stop_words and len(w)>=3 and not(w.isnumeric()) :\n",
        "            filtered_tokens.append(w)\n",
        "    return filtered_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6vz73yUv9Ih",
        "outputId": "77c7a4d8-a995-4e3d-9ba5-43660d3fa8a9"
      },
      "source": [
        " import nltk\n",
        " nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMMwICexv_ye"
      },
      "source": [
        "for idx in range(len(df)):\n",
        "  df.iloc[idx,0]=np.array(filter_tokens(df.iloc[idx,0]))\n",
        "for i in range(len(df)):\n",
        "    df.iloc[i,0]=' '.join(df.iloc[i,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TSH37epK-he"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQkx_4qB0Z7m",
        "outputId": "90282be1-ed31-4056-81df-efc1d0e93f71"
      },
      "source": [
        "df.review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40877    recently started watching show say really made...\n",
              "18057    return jedi often remembered wrong rather righ...\n",
              "19066    remember loved movie came years old commodore ...\n",
              "20525    know last reviewer talking show pure entertain...\n",
              "5847     beginning excited see movie poster possibly fu...\n",
              "                               ...                        \n",
              "11284    shadow magic recaptures joy amazement first mo...\n",
              "44732    found movie quite enjoyable fairly entertainin...\n",
              "38158    avoid one terrible movie exciting pointless mu...\n",
              "860      production quite surprise absolutely love obsc...\n",
              "15795    decent movie although little bit short time pa...\n",
              "Name: review, Length: 45000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cATtFvduTv6q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGnvUCRLzH1r"
      },
      "source": [
        "max_features = 5000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(df['review'].values)\n",
        "X = tokenizer.texts_to_sequences(df['review'].values,)\n",
        "X = pad_sequences(X)\n",
        "\n",
        "# reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "# # Function takes a tokenized sentence and returns the words\n",
        "# def sequence_to_text(list_of_indices):\n",
        "#     # Looking up words in dictionary\n",
        "#     words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "#     return(words)\n",
        "\n",
        "# # Creating texts \n",
        "# df.review = list(map(sequence_to_text, X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPTQu_SPvi_V"
      },
      "source": [
        "# Load Pretrained Word2Vec\n",
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vFoOiW3vzqP"
      },
      "source": [
        "def get_word2vec_enc(reviews):\n",
        "    \"\"\"\n",
        "    get word2vec value for each word in sentence.\n",
        "    concatenate word in numpy array, so we can use it as RNN input\n",
        "    \"\"\"\n",
        "    encoded_reviews = []\n",
        "    for review in reviews:\n",
        "        word2vec_embedding = np.array(embed(review))\n",
        "        encoded_reviews.append(word2vec_embedding)\n",
        "    return encoded_reviews\n",
        "\n",
        "\n",
        "def get_padded_encoded_reviews(encoded_reviews):\n",
        "    \"\"\"\n",
        "    for short sentences, we prepend zero padding so all input to RNN has same length\n",
        "    \"\"\"\n",
        "    padded_reviews_encoding = []\n",
        "    for enc_review in encoded_reviews:\n",
        "        zero_padding_cnt = 640 - enc_review.shape[0]\n",
        "        pad = np.zeros((zero_padding_cnt, 250))\n",
        "        enc_review = np.concatenate((pad, enc_review), axis=0)\n",
        "        padded_reviews_encoding.append(enc_review)\n",
        "    return np.array(padded_reviews_encoding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oXFy9qOwUXZ"
      },
      "source": [
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = np.zeros((vocab_size, 250))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tweight_matrix[i] = embedding([word])\n",
        "\treturn weight_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1PWYcayOay3"
      },
      "source": [
        "weight_matrix=get_weight_matrix(embed, tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo7MVe5uIiUc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnExpzY3w34y"
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tfidf_model = TfidfVectorizer(token_pattern=r\"\\w+['\\w]*\",ngram_range=(1,2),min_df=0.01,max_df=0.99) # specify parameters here\n",
        "# tfidf_model.fit(data['review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwRKkSqkyVhf"
      },
      "source": [
        "# raw_data_tfidf=tfidf_model.transform(data['review'])\n",
        "# vocab=tfidf_model.vocabulary_\n",
        "\n",
        "# X=raw_data_tfidf\n",
        "# X=X.toarray()\n",
        "# X=[sorted(X[idx],reverse=True)[:20] for idx in range(len(X))]\n",
        "# X=np.array(X)\n",
        "# X = np.expand_dims(X, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7q2jK0Z5Hzo"
      },
      "source": [
        "# print('Preparing embedding matrix')\n",
        "# EMBEDDING_DIM = 250\n",
        "# embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIM))\n",
        "# for word, i in vocab.items():\n",
        "#         embedding_matrix[i] = np.array(embed([word]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAa996C23k-"
      },
      "source": [
        "# trainsum=np.sum(raw_data_tfidf.toarray(),axis=1)\n",
        "# w2v_tfidf=raw_data_tfidf.dot(embedding_matrix)\n",
        "# for i in range(len(data)):\n",
        "#     w2v_tfidf[i]=w2v_tfidf[i]/trainsum[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z2GyG6SNbpE"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjdpop9AylBO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybkpC-STZVdn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VaEDZfYODQ8"
      },
      "source": [
        "# X=[sorted(X[idx],reverse=True)[:20] for idx in range(len(X))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuIPXGPaAVNZ"
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(LSTM(units=64,input_shape=(20, 1 ),dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(2,activation='softmax'))\n",
        "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QLO7VR2Mj3m",
        "outputId": "1b1e483f-aa9b-443d-8298-1902e518c0d9"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le=preprocessing.LabelEncoder()\n",
        "Y=le.fit_transform(df['sentiment'])\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33750, 909) (33750,)\n",
            "(11250, 909) (11250,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdBOQLS5P7Db"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "X_train=pd.DataFrame(data=X_train[0:,0:],\n",
        "    index=[i for i in range(X_train.shape[0])],\n",
        "    columns=['f'+str(i) for i in range(X_train.shape[1])])\n",
        "Y_train=pd.DataFrame(data=Y_train)\n",
        "\n",
        "df = pd.concat([X_train, Y_train], axis=1)\n",
        "\n",
        "\n",
        "# Separate majority and minority classes\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df.iloc[:,18]==0]\n",
        "df_minority = df[df.iloc[:,18]==1]\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=1234) # reproducible results\n",
        " \n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        " \n",
        "# Display new class counts\n",
        "X_train=np.array(df_upsampled.iloc[:,:18])\n",
        "Y_train=np.array(df_upsampled.iloc[:,18])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HEsqqSMx56a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c86eab-9b37-4630-af64-3f924e25c17e"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  234,  500,  389],\n",
              "       [   0,    0,    0, ...,    6,    1,  654],\n",
              "       [   0,    0,    0, ...,    1,   50,  205],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  976, 1468,  452],\n",
              "       [   0,    0,    0, ...,  147,  131,   27],\n",
              "       [   0,    0,    0, ...,  345,   13, 1820]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E-w9MgbSdHz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQoPMV2bMkEI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "outputId": "7f0993c6-3b6b-4756-a657-f6a8a0563037"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "# embedding_layer = Embedding(vocab_size, 250, weights=[weight_matrix], input_length=909, trainable=False)\n",
        "embedding_layer = Embedding(vocab_size, 128,  input_length=909)\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "batch_size = 32\n",
        "\n",
        "checkpoint = ModelCheckpoint('model_best_weights.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', period=1)\n",
        "\n",
        "model.fit(X_train, Y_train, epochs =5,validation_data = (X_test, \n",
        "Y_test), callbacks=[checkpoint] ,batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 909, 128)          12585728  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 905, 64)           41024     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 452, 64)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 28928)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                289290    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 12,916,053\n",
            "Trainable params: 12,916,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/3\n",
            "1055/1055 - 150s - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.4977\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.49769, saving model to model_best_weights.h5\n",
            "Epoch 2/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ffe0a279057a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m model.fit(X_train, Y_train, epochs =3,validation_data = (X_test, \n\u001b[0;32m---> 38\u001b[0;31m Y_test), callbacks=[checkpoint] ,batch_size=batch_size, verbose = 2)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI59P_77gYgW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TcakUNrgYn5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM2SUXkxgYsP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVXsR5XTgYze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H6q_yljgY6j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yo2bJxAgZBq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTvewx8gZJW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpw9FV8GgZRi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQo2hcdVgZZ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d1OmF_pgZij"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmI7GtBzgZrn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10QoY3cSgZ1g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvoqbCC7gaAD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZiVFZoVgaJS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c086EAAIgaTB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WGiH6udgab7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYV6IotugalY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaiBUsQYgaul"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frQrC1vjga4J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI4CAQ20gbBj",
        "outputId": "329d78b8-b0be-4b68-f08d-8bd6b28d56bf"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40877    I recently started watching this show, and I h...\n",
              "18057    \"Return of the Jedi\" is often remembered for w...\n",
              "19066    I remember I loved this movie when it came out...\n",
              "20525    I don't know what the last reviewer is talking...\n",
              "5847     From the very beginning I was so excited to se...\n",
              "                               ...                        \n",
              "11284    `Shadow Magic' recaptures the joy and amazemen...\n",
              "44732    I found this movie to be quite enjoyable and f...\n",
              "38158    Avoid this one! It is a terrible movie. So wha...\n",
              "860      This production was quite a surprise for me. I...\n",
              "15795    This is a decent movie. Although little bit sh...\n",
              "Name: review, Length: 45000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqvjPnucgbLb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoPDgxWUAY5z",
        "outputId": "ae1419ab-bea2-474f-eb46-4c1d29c0eb79"
      },
      "source": [
        "embed_dim = 128\n",
        "# vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(2000, 250,input_length=909, trainable=False)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,input_shape=(909,250)))\n",
        "model.add(Dense(128,kernel_initializer='HeNormal', input_dim=196,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 909, 250)          500000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 909, 196)          350448    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 909, 128)          25216     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 909, 1)            129       \n",
            "=================================================================\n",
            "Total params: 875,793\n",
            "Trainable params: 375,793\n",
            "Non-trainable params: 500,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6APbYRKFK1a5",
        "outputId": "bbd8f694-9bc0-4fc3-d4be-817ce31d53f3"
      },
      "source": [
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "batch_size = 32\n",
        "\n",
        "checkpoint = ModelCheckpoint('model_best_weights.h5', monitor='val_accuracy', verbose=2, save_best_only=True, mode='max', period=1)\n",
        "\n",
        "model.fit(X_train, Y_train, epochs =10,validation_data = (X_test, \n",
        "Y_test), callbacks=[checkpoint] ,batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV8E7LHyUNew"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.models import load_model\n",
        "\n",
        "# load model\n",
        "model = load_model('model_best_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi-lTRaRt0G5",
        "outputId": "6c942116-8961-4717-9db1-cf64bf53352c"
      },
      "source": [
        "model.evaluate(test,y_t,batch_size=32,verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 - 1s - loss: 0.3012 - accuracy: 0.8818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3012043833732605, 0.8817999958992004]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKhZNaE4uziY",
        "outputId": "60fd64f5-3450-47f4-8e45-c7872219ae47"
      },
      "source": [
        "roc_auc_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7402548840048839"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY7BTfEPu6yD",
        "outputId": "10d85733-04af-4940-8d60-d81547718c92"
      },
      "source": [
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "111/111 - 1s - loss: 0.6869 - accuracy: 0.8226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAOcp6gk7x5Y",
        "outputId": "c28d150a-4ac1-462f-be0d-fe62ad349e93"
      },
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for idx in range(len(X_test)):\n",
        "    \n",
        "    result = model.predict_classes(X_test[idx].reshape(1,X_test.shape[1]))[0][0]\n",
        "   \n",
        "    if result == Y_test[idx]:\n",
        "        if result == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if Y_test[idx] == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pos_acc 64.58333333333334 %\n",
            "neg_acc 85.32435306628855 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBmUnWgA7zfT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA9HF5smHYkU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HziWItjeIdDL",
        "outputId": "7e0e30c8-9f52-43e9-8edf-ff6ea70bd546"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1BiQb2gIjDJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHPOOyoHJUnq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}